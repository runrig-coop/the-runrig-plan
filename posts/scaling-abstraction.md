---
title: Scaling and Abstraction
subtitle: A subtitle
description: A description.
author: Jamie Gaehring
date: 2025-07-29
---

In Silicon Valley, there is a widespread fascination with _scaling_, or to be
more precise, _digital technologies that scale_. The verb "to scale" in this
context can take the passive voice, as in digital technologies that "can be
scaled," or an active voice for technologies that facilitate "the scaling of"
other systems, both digital and non-digital systems alike. If some new tech
promises "to scale" and "to be scaled" at the same time, all the better. Many a
would-be founder has exalted the properties of this or that technology for its
ability to scale, as if by some quasi-magical property latent in the computer
chips themselves. But scaling is by no means inherent to the nature of
computation, nor does scaling emerge from digital technology all of its own
accord. Rather, it is imposed on technology by a mandate from venture capital
investors to pursue unlimited economic growth.

For its own part, information technology does make an original contribution in
the form of its unrivaled capacity for _abstraction_, a power that can just as
well be applied to scaling as to other unrelated tasks or even opposing aims. A
well-designed computer algorithm can abstract away concrete details of the real
world – e.g., material goods and services, users, workers, facial expressions,
social relations, monetary costs, or environmental costs – and whisk them away
to the _cloud_. Once in this realm of pure abstraction, properties like color,
size, and shape become mere numbers or bits. Free of all physical encumbrance,
our worldly cares assume new virtual bodies, becoming weightless, untethered,
and without consequence. Up there in the cloud, scale itself is only limited to
the largest number you can fit into a [64-bit register] – although that limit,
too, can be easily abstracted away.

When the object of scaling is economic productivity or market dynamics,
computational abstraction becomes an accelerant for capital's race towards
infinite growth. This secret sauce – abstraction coupled to a business model
meant for rapid market growth and capital accumulation – is what business
analysts or techno-optimists typically infer by the neologism: _scalability_.

Before going too much further, I should point out that in addition to the
economic sense of the word, scalability has another more technical usage, where
scale can be measured by empirical observation or even evaluated by mathematical
proof.[^comp-sci] Some might argue that this technical meaning can be considered
independently from its business connotation, without any reference to
socioeconomic value statements. That may be true in a purely academic setting,
but even under such contrived circumstances, I would argue that the term still
comes freighted with some heavy socioeconomic implications. The physical limits
and potentials of scalability measured in the laboratory are of interest largely
to the extent that they can be correlated to the economic costs and benefits of
scalability. There is no applied science for the upper reaches of scalability
without the vast resources available only to tech companies intent on scaling to
billions of users and a market capitalization that puts them on the S&P 500.
That's what funds scientific research into scalability in the first and why it
gets any significant attention.

[^comp-sci]: Scalability is a pretty dry body of literature in the applied
    sciences, but to get a sense, see [Amdahl's Law] and [Gunther's Universal
    Scalability Law]. The theoretical physics behind computational limits is
    actually a lot more approachable and fun to explore. On her YouTube channel
    _Up and Atom_, Jade Tan-Holmes gives a fantastic explanation of ["Why Pure
    Information Gives Off Heat"] according to [Landauer's Principle]. To
    understand how Planck's constant and the Uncertainty Principle put a hard
    upper limit on how much information can be transmitted over a fixed period
    of time, see ["What is the maximum Bandwidth?"] with  Prof. Mike Merrifield
    and Brady Haran from _Sixty Symbols_. It's far more useful, in my opinion,
    to get a beginner's intuition for the _physicality of information_ than to
    memorize a bunch of equations for scaling systems that have no business
    being that big to start with.

[Amdahl's Law]: https://dl.acm.org/doi/10.1145/1465482.1465560
[Gunther's Universal Scalability Law]: https://arxiv.org/abs/0808.1431v2
[Landauer's Principle]: https://en.wikipedia.org/wiki/Landauer%27s_principle
["Why Pure Information Gives Off Heat"]: https://www.youtube.com/watch?v=XY-mbr-aAZE
["What is the maximum Bandwidth?"]: https://www.youtube.com/watch?v=0OOmSyaoAt0

To that point, it remains to be seen if the abstractions of scalability can
survive eventual contact with reality – _the cloud falling back down to earth_,
so to speak. Computational abstractions do incur physical costs and real-world
consequences, and there are practical limits to the scale of their application,
even if they encompass theoretical infinities. The need for sane limits on
computational scaling could not be more acute than in the face of our
accelerating climate crisis and the rising number of geopolitical conflicts
spawned by competition over finite energy and mineral resources. Indeed, such
resources will never be adequate to the computational demands of today's tech
moguls, if left to set their own limits. When Microsoft announces [it will
reopen Three Mile Island] to power its large language models, _this is the cloud
falling back down to earth_. When companies like Apple, Tesla and Dell are
willing to pay millions of dollars in legal fees each year so they can keep
[extracting the conflict minerals] that power our smartphones, electric
vehicles, and other devices, _this too is the cloud falling back down to earth_.

[it will reopen Three Mile Island]:
    https://www.npr.org/2024/09/20/nx-s1-5120581/three-mile-island-nuclear-power-plant-microsoft-ai
[extracting the conflict minerals]:
    https://arstechnica.com/tech-policy/2024/03/apple-and-other-firms-dont-have-to-compensate-victims-of-forced-child-labor/

Beyond the clear perils to our planet's climate and natural ecosystems, rapid
scaling also threatens our social and cultural ecosystems. When big tech
companies talk about scalability, they might have in mind scaling the production
of our physical needs and wants (e.g., GrubHub, Apple, Tesla), scaling the
market for those products (e.g., Amazon, AdSense, Square), scaling the creative
arts and our cultural identities (e.g., Netflix, Spotify, YouTube), or scaling
the web of social relations bound up in all of that (e.g., Facebook, LinkedIn,
Tinder). But when these processes are scaled by algorithmic abstraction, some
essential quality of our social relations will always be lost.

In many ways, abstraction is just the omission of certain characteristics that
make real-world phenomena especially inscrutable to meaningful analysis. Those
details perceived as anomalous, divergent, or simply irrelevant are thrown out
while other patterns and traits are elevated in their place. All of this is done
to form a coherent model of whichever dynamics the modeler deems most
significant. As George Box once put it, "all models are wrong, but some are
useful," and abstraction can just as easily produce models that are insightful
and beneficial to society as it can throw up models that are misleading,
exploitative, or utterly meaningless. In the case of most cloud software, the
abstraction is performed by proprietary algorithms, hidden away on a remote
server somewhere that only its owners can ever see or control. Just ask any
content creator who's tried to guess what thumbnail image will get them the most
views, or an SEO consultant who's racked their brain for the right combination
of keywords to improve their website's search ranking: they'll tell you just how
futile a guessing game that can be.

Billions of decisions are being made every second on the basis of such
cloud-based abstractions, and all for the sake of somebody's model. But _whose_?
Most of those decisions are the sole prerogative of the algorithm's authors,
while the overwhelming majority of us are relegated to being the mere _objects
of their abstractions_, even if we never use the particular cloud software in
question. Users and non-users alike are seldom granted any knowledge of the
decisions being made that impact our lives, let alone any influence over how
those abstractions are formed in the first place. When the phenomenon being
abstracted away is an entire economic sector or, worse yet, society as a whole,
we forfeit a tremendous degree of agency over our social lives and our very
material existence. All that power of abstraction is essentially handed over to
just a few over-caffeinated engineers and their even fewer corporate managers.
Once in their hands they'll do whatever they deem necessary for the sake of
scale, often to the detriment of our communities and ultimately to the sole
benefit of their shareholders.

That imbalance of control is the quantity that so much of our modern tech
infrastructure is specifically designed to scale up. Another implicit assumption
of scalability is that however a company's market share is measured to rise –
e.g., the number of active users, payments processed or total revenues – that
value is expected to scale at a _geometric_ rate with respect to the number of
engineers and managers it must hire to achieve the increase. A mere _linear_
rate of growth is likely to be seen as an abject failure. Whether the key metric
is expressed in users or dollars, if it's to be compared against the number of
employees or other operating costs, it must indicate some unequal exchange of
socioeconomic value. In other words, _scalability is often just a measure of
social control_.

Social control is nothing new, of course, and by no means requires computational
scalability. Ancient bureaucrats did just fine with their law books, abacuses,
_quipu_, and clay tablets for millennia before its invention. In fact, the
degree of control provided by this form of scaling is so far in excess of what
those engineers, managers, and shareholders can ever hope to exercise, at least
with anything like conscious intent. As with many forms of industrialization, it
is the overwhelming bluntness of the tool, the haphazard way it is wielded, and
the comparably narrow scope of its objectives that makes scaling technologies
capable of such unprecedented harm. Like an ocean trawler that obliterates
square miles of seafloor habitat and all the marine life it sustains, just to
throw out three quarters of its catch in the end, scalability can wreak utter
havoc across wide swathes of our society, wasting untold resources, while its
operators scarcely pay any notice.

It should go without saying that scaling and abstractions of this sort are
directly at odds with any vision for appropriate technology that respects users'
individual or collective autonomy.

Runrig's contention is this: technology can aid in our collective liberation,
but only by strengthening our economic, social, and ecological relationships
_first_, before deciding on any abstractions or scaling algorithms or data
models. Once we've established the necessary cohesion for collective decision
making, then, and only then, can we begin to scale our systems of economic and
social reproduction. This can be done incrementally, starting off with very
small communities or social units that grow larger over time as each iteration
expands the capacity for collective action. Don't let it be misconstrued that
only once we're living in perfect peace and harmony can we overthrow capitalism!

Digital technology can facilitate liberation at any step along the way, but
organizers would be wise to adopt only those technologies that they control and
they can maintain by their own means and under their own power, even at the
earliest phases of organizing. Community control as a prerequisite to adoption
would be an automatic limiting factor to both scale and functionality, because
maintaining technology independently does come at a cost, especially if you are
not relying on any existing proprietary software or corporate cloud services.
However, this should be seen as a healthy, _humanizing_ dose of parsimony, not a
shortcoming. After all, the costs are shared by the entire community, not each
individual left to their own devices. That spreads out the costs, of course, but
it also reduces or eliminates certain costs by pooling the community's
collective resources, both monetary and non-monetary. Someone might have
technical skills they can share towards the development or maintenance of some
technology. Others might have an old laptop they were about to throw out that
could be repurposed for a server. Meanwhile, someone else may have a little
extra time to perform a simple manual task that obviates the need to run some
little bit of costly technology altogether. These savings may not add up to
much, they may not be contributed uniformly among all members, and some people
may find it easier to toss a little extra cash in the pot, all of which is fine.
Pooling shared resources like this, however, does double duty of strengthening
social bonds and builds the collective capabilities, and if it saves a few
bucks, all the better! It all goes straight back into a virtuous cycle that
becomes less and less constrained by financial costs and more and more
autonomous over time.

Much of this is just a crude restatement of mutual aid's basic tenets, and their
application to food and technology is hardly anything new. I bring them up here
because they address specific critiques about free software and cooperative
technology, namely that it cannot be developed sustainably without accepting
venture capital, or least by adopting some of its methods like raising capital
and scaling production. Skepticism like this is common even within open source
communities, and it is understandable. The ideals of software freedom and
cooperativism must be balanced against guaranteeing that software developers and
maintainers are compensated fairly for their labor and that users are provided
with secure and reliable applications – preferably really nice applications! –
and that's no trivial nut to crack. What alternatives do exist are poorly
attested within open source circles, and I'd say there's a lack of rigorous
arguments that go deeply enough into addressing specific, practical concerns.
That's the task Runrig aims to fulfill.


|            Runrig Methodology |     | Venture Capitalism      |
| ----------------------------: | --- | :---------------------- |
|    ___Distributing_ CONTROL__ | vs. | _Scaling PRODUCTION_    |
|         ___Diffusing_ COSTS__ | vs. | _Accumulating CAPITAL_  |
| ___Expanding_ PARTICIPATION__ | vs. | _Consolidating MARKETS_ |
|       __WORKER-_organizing___ | vs. | _RENT-seeking_          |


Original formulation:
> - Organize __support__ and __reach__
>     - vs. _capital accumulation_ and _marketing_
> - Distribute __costs__ and __control__
>     - vs. _scaling_ and _infinite growth_


Technology does not have to be scalable in order to help large numbers of people,

Technology does not have to be scalable, nor is it obligated to _scale our
social and economic production_ systems, all for the sake of making our
communities more efficient machines for _capital accumulation_. We can choose to
__distribute control__ of our economic and social production more evenly, while
also __diffusing the costs__ of maintaining those systems. Instead of
_consolidating markets_ into fewer and fewer hands, we can __expand the zone of
participation__, along with our capacity for collective action. And although we
still live under capitalism where technocratic _rent-seeking_ pervades our
economic lives whether we have a Facebook account or not, we can choose not to
reproduce those dynamics with our labor and technology. Instead, let's use them to
__organize bulwarks of worker power__ that are more resilient to rent-seeking
efforts and other forms of attack.






[64-bit register]:
    https://tromp.github.io/blog/2023/11/24/largest-number


## Draft Notes
### Playing w/ $\LaTeX$

$$
\frac {\Delta \textit{Attn}^\textit{Scal.}} {\Delta \textit{Tech}} \times \textit{Operating Capital}
$$

Or...

$$
\Delta L_\textit{Attn} = \Delta (L_\textit{Tech})^\textit{Scale}
$$

Or...

$$
\textit{Scale} = \log_{(\Delta L_\textit{Tech})}(\Delta L_\textit{Attn})
$$

Or...

$$
Scalability = \frac {\Delta L_a^S} {\Delta L_t}
$$

Or...

$$
Scalability = \frac{\log_{S}L_{Attn}}{L_{Tech}} 
$$


Or...

$$
\frac {\Delta L_\textit{Attn.}F_\textit{Scal.}} {\Delta L_\textit{Tech.}} \times Capital
$$

Original example:

$$
\sum_{k=3}^5 k^2=3^2 + 4^2 + 5^2 =50
$$




### scratch
> Civilization advances by extending the number of important operations which we
can perform without thinking about them.

![](https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/AreWeThereYet/00.12.08.jpg?raw=true)

$3^2$

$3 \times 3$

$3 + 3 + 3$


- Scaling == Measure of User-Share or Market-Share
- Growth == Measure of Company Valuation or Market Capitalization


I prefer to think about the ways technology can help to distribute control over our production systems, while also diffusing the costs of maintaining them. From this perspective, it’s not the product that expands to an ever greater and greater scale, but rather our capacity to organize a larger bloc of mutual support with a stronger commitment to shared values.

In very practical terms, this is about maximizing the contributions that can be integrated into a project from an ever wider base of members. That might take the form of extending a piece of open source software created by another project, rather than starting off on one’s own to recreate their work, or it could mean empowering technical service providers with access to open source tools so that they can in turn contribute their unique insights to the future development of those tools. This is cooperation being favored over competition, in essence. In terms of technology development and especially technology maintenance, widening the circle of cooperation can diffuse costs, by lowering the stakes required to initiate development at the very start, while also averaging out the long-term costs of ownership through shared ownership.

The sooner you can get to a stage of organizing with potential allies to decide on the next course of action, the better situated and resourced you will be to act, and with greater assurances those actions won’t exceed your capacity. It won’t ever be a source of passive income for any one person, but it will add value to the labor and expertise of all those who contribute to the project.


With regard to engineers, managers and users, _scalability_ also implies a
distinct relationship between the number of fulltime salaried professionals a
company employs versus the number of active users it boasts or some other
measure of traffic it can verifiably sustain. It's not the absolute number of
employees or user traffic that matters so much but _the rate at which both
increase with respect to each other_.

The ratio of these two rates of change is yet another form of scalability,
except it is beyond the control of even the highest paid engineers and managers,
perhaps even the company's founders and executives.

a higher order of abstraction meant to scale scalability itself,

While the growth of the startup's labor force is generally
taken to be a healthy indicator, the goal is still to keep labor costs as low as
possible even as the overall market-share of users the startup can command grows
exponentially in proportion. Mere linear growth would in fact be tantamount to a
startup's _failure_ to scale. Venture capital investors count on a startup's
user-share rising _geometrically_ as labor costs rise only _arithmetically_.
That is scalability's essential characteristic, or at least its most vaulted
claim. It's what most entices VC firms to invest, and consequently lies behind
the startup's fundamental motivating drive: to maximize the rate at which it can
capture _users' unpaid attention labor_ with respect to how much _paid technical
labor_ it must first extract from its workers in order to do so. Scalability,
defined as such, is nothing less than the Platonic ideal of upwards wealth
transfer and the consolidation of power.




It should go without saying that unlimited scaling of this sort is at odds with
the goal of producing technology that affords its users greater autonomy. Again,
this is not so much an inherent trait of computers or algorithms alone, but
rather the economic models driving the expansion of that technology.

Alternatively, as Runrig contends, digital technology can be made into a driver
for distributing control of our economic and social production far more broadly,
while also diffusing the costs of maintaining them. Rather than scaling those
systems for their own sake, as mere machines for capital accumulation, we can
expand the zone of participation, along with our capacity for collective action.
Instead of consolidating markets into the fewer and fewer hands, we can organize
our communities for mutual support, strengthening our bonds of interdependence.
The limited use of technology can help strengthen our relationships among other
workers




Free software – that is, software that users are free to share, modify, and use
however they see fit – can be far more than an ethos when done right. It can be
a powerful strategy for organizing limited resources and disparate contributors
to develop effective software that can far exceed what's possible with
proprietary software, even with massive VC funding (eg, Wikipedia). However, it
requires putting aside a few assumptions that can spill over from proprietary
business models.

Part of Runrig's methodology is the identification of a few of these anti-patterns, most prevalent within VC-funded startup culture of Silicon Valley, which we recommend to all free software projects
in search of a pathway to stable development and long-term maintenance, is to 

|            Runrig Methodology |     | Venture Capitalism      |
| ----------------------------: | --- | :---------------------- |
|    ___Distributing_ CONTROL__ | vs  | _Scaling PRODUCTION_    |
|         ___Diffusing_ COSTS__ | vs  | _Accumulating CAPITAL_  |
| ___Expanding_ PARTICIPATION__ | vs  | _Consolidating MARKETS_ |
|    __COMMUNITY-_organizing___ | vs  | _RENT-seeking_          |


Technology does not have to be infinitely scalable, nor is it obligated to scale
our social and economic production systems merely the sake of making our
communities more efficient machines for capital accumulation. We can choose to
__distribute control__ of our economic and social production more evenly, while
also __diffusing the costs__ of maintaining those systems. Instead of
consolidating markets into fewer and fewer hands, we can __expand the zone of
participation__, along with our capacity for collective action. And although
digital rent-seeking pervades our networked lives, regardless of whether we
choose to engage with many of those networks, we can still choose not to
reproduce those dynamics in our own work and the tools we build for ourselves.
Instead, let's use them to __organize bulwarks of community power__ that are
more resilient to rent-seeking efforts and other forms of attack.


[^scarcity]: Natural resources are indeed _finite_, in strictly physical terms,
but their _scarcity_ only emerges within a social context: they are scarce
because society's total demand exceeds what it is capable of supplying
universally to all its members. In other words, scarcity is just demand
outstripping supply, basic Econ 101, but supply and demand are both socially
determined quantities. To be sure, certain physical limits put constraints on
precisely _how_ that determination can or cannot be made, but all agency to
determine whether society suffers under scarcity or thrives in abundance belongs
to society itself, not to physical law.

